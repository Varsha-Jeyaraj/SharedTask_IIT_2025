{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Varsha-Jeyaraj/SharedTask_IIT_2025/blob/main/Thamizhi_FT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" huggingface_hub hf_transfer\n",
        "    !pip install --no-deps unsloth"
      ],
      "metadata": {
        "id": "0lOcC4z8XFEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "dAbX1_qhXLy3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%env UNSLOTH_RETURN_LOGITS=1"
      ],
      "metadata": {
        "id": "5PCvdA3kYMYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nkc_9lCEUOO5"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "print(\"Mounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "max_seq_length = 2048\n",
        "dtype = None\n",
        "load_in_4bit = True\n",
        "\n",
        "\n",
        "cpt_model_path = \"/content/drive/MyDrive/My_Models/CPT_Model_Iter1\"\n",
        "\n",
        "print(f\"\\nLoading the CPT model from: {cpt_model_path}\")\n",
        "print(\"This model has your trained LoRA adapters separate from the base model.\")\n",
        "\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = cpt_model_path,\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")\n",
        "\n",
        "print(\"\\n--- CPT Model and Adapters Loaded Successfully! ---\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "sft_dataset = load_dataset(\"abhinand/tamil-alpaca\", split=\"train\")"
      ],
      "metadata": {
        "id": "HgOwjYA1XDns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sft_dataset[9])"
      ],
      "metadata": {
        "id": "PpopPhnObaBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_alpaca_prompt = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "# Becomes:\n",
        "alpaca_prompt = \"\"\"\n",
        "ஒரு பணியை விவரிக்கும் ஒரு வழிமுறை கீழே உள்ளது. கோரிக்கையை சரியாக பூர்த்தி செய்யும் பதிலை எழுதுங்கள்.\n",
        "\n",
        "### பணி:\n",
        "{}\n",
        "\n",
        "### பதில்:\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "def formatting_prompts_func(examples):\n",
        "    instructions = examples[\"instruction\"]\n",
        "    inputs       = examples[\"input\"]\n",
        "    outputs      = examples[\"output\"]\n",
        "    texts = []\n",
        "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
        "        text = alpaca_prompt.format(instruction, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts, }\n",
        "pass\n",
        "\n",
        "sft_dataset = sft_dataset.map(formatting_prompts_func, batched = True,)"
      ],
      "metadata": {
        "id": "wu9H9WcJcnle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "alpaca_prompt_template = \"\"\"\n",
        "ஒரு பணியை விவரிக்கும் ஒரு வழிமுறை கீழே உள்ளது. கோரிக்கையை சரியாக பூர்த்தி செய்யும் பதிலை எழுதுங்கள்.\n",
        "\n",
        "### பணி:\n",
        "{}\n",
        "\n",
        "### உள்ளீடு:\n",
        "{}\n",
        "\n",
        "### பதில்:\n",
        "{}\"\"\"\n",
        "\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "\n",
        "\n",
        "def formatting_prompts_func_for_tamil_alpaca(example):\n",
        "    instruction = example[\"instruction\"]\n",
        "    input_text  = example[\"input\"]\n",
        "    output      = example[\"output\"]\n",
        "\n",
        "    text = alpaca_prompt_template.format(instruction, input_text, output) + EOS_TOKEN\n",
        "\n",
        "    return { \"text\": text }\n",
        "pass\n",
        "\n",
        "\n",
        "print(\"Formatting the SFT dataset...\")\n",
        "sft_dataset = sft_dataset.map(formatting_prompts_func_for_tamil_alpaca,)\n",
        "print(\"Dataset formatted successfully.\")\n",
        "\n",
        "\n",
        "print(\"\\nHere is an example of a formatted prompt:\")\n",
        "print(sft_dataset[0]['text'])"
      ],
      "metadata": {
        "id": "NNnJsdXqed4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "from unsloth import UnslothTrainer, UnslothTrainingArguments\n",
        "\n",
        "sft_output_dir = \"/content/drive/MyDrive/My_Models/SFT_Checkpoints\"\n",
        "\n",
        "trainer = UnslothTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = sft_dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 8,\n",
        "\n",
        "    args = UnslothTrainingArguments(\n",
        "        save_strategy = \"steps\",\n",
        "        save_steps = 50,\n",
        "        per_device_train_batch_size = 4,\n",
        "        gradient_accumulation_steps = 4,\n",
        "\n",
        "        max_steps = 120,\n",
        "        warmup_steps = 10,\n",
        "        num_train_epochs = 5,\n",
        "\n",
        "        learning_rate = 5e-5,\n",
        "        embedding_learning_rate = 1e-5,\n",
        "        fp16 = True,\n",
        "\n",
        "\n",
        "        logging_steps = 10,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        remove_unused_columns = False,\n",
        "        output_dir = sft_output_dir,\n",
        "        report_to = \"none\",\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "zvY4YNW2e7Ts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_stats = trainer.train()"
      ],
      "metadata": {
        "id": "zifYETI9gXBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "max_memory = round(torch.cuda.get_device_properties(0).total_memory / 1024 / 1024 / 1024, 3)\n",
        "start_gpu_memory = torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024\n",
        "\n",
        "\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ],
      "metadata": {
        "id": "f7LIM_jn5Zsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FastLanguageModel.for_inference(model)\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt_template.format(\n",
        "        \"தமிழ் இசை பற்றி 50-75 வார்த்தைகளுக்குள் ஒரு கட்டுரை தருக.\", # instruction\n",
        "        \"\",#input\n",
        "        \"\"# output\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)"
      ],
      "metadata": {
        "id": "cmUWOVyZGm51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_model_path_drive = \"/content/drive/MyDrive/My_Models/SFT_Model_Final_16bit\"\n",
        "print(f\"\\nSaving merged 16-bit model to Google Drive at: {final_model_path_drive}\")\n",
        "model.save_pretrained_merged(final_model_path_drive, tokenizer, save_method = \"merged_16bit\")\n",
        "print(\"Save to Drive complete.\")"
      ],
      "metadata": {
        "id": "cvtfpw9SG-N8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "hf_write_token = \"HF_TOKEN2\"\n",
        "\n",
        "\n",
        "repo_id = \"Thamizhi-Tamil-BaseModel-Parameters-FT\"\n",
        "\n",
        "\n",
        "final_model_path_drive = \"/content/drive/MyDrive/My_Models/SFT_Model_Final_16bit\"\n",
        "model.save_pretrained_merged(final_model_path_drive, tokenizer, save_method = \"merged_16bit\")\n",
        "\n",
        "\n",
        "print(f\"Uploading merged model to Hugging Face Hub: {repo_id}\")\n",
        "model.push_to_hub_merged(repo_id, tokenizer, save_method = \"merged_16bit\", token = hf_write_token)\n",
        "print(\"Upload to Hub complete!\")"
      ],
      "metadata": {
        "id": "KWn8m7cq9Iio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w9C4N2ZRG928"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}