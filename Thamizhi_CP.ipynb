{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbLCry4nYleB"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" huggingface_hub hf_transfer\n",
        "    !pip install --no-deps unsloth"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "SlnnCr9vZAva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%env UNSLOTH_RETURN_LOGITS=1"
      ],
      "metadata": {
        "id": "oZ9cM9IKZJSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048\n",
        "dtype = None\n",
        "load_in_4bit = True\n",
        "\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Phi-3-mini-4k-instruct\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")"
      ],
      "metadata": {
        "id": "tFveXipmZRNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompts_to_test = [\n",
        "    # --- In-Domain (Tamil) Prompts ---\n",
        "    \"திருக்குறளை எழுதியவர் யார்?\",\n",
        "    \"பொங்கல் பண்டிகை பற்றி ஒரு பந்தி எழுதுக.\",\n",
        "    \"கோவிட்-19 பற்றி 75-100 வார்த்தைகளுக்குள் ஒரு கட்டுரை தருக.\",\n",
        "\n",
        "    # --- Out-of-Domain (Sanity Check) Prompts ---\n",
        "    \"What is the capital of Japan?\",\n",
        "    \"Write a short sentence about a computer.\"\n",
        "]\n",
        "\n",
        "print(\"--- Running 'Before Training' Evaluation ---\")\n",
        "\n",
        "\n",
        "for prompt in prompts_to_test:\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    outputs = model.generate(**inputs, max_new_tokens=100)\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "    print(f\"Response: {response}\\n\")\n",
        "    print(\"-\" * 30,\"\\n\")"
      ],
      "metadata": {
        "id": "N8bhTy7pb2Mt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 128,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "\n",
        "                      \"embed_tokens\", \"lm_head\",],\n",
        "    lora_alpha = 32,\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        "    use_rslora = True,\n",
        "    loftq_config = None,\n",
        ")"
      ],
      "metadata": {
        "id": "EqeatJVkggrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EOS_TOKEN = tokenizer.eos_token\n",
        "def formatting_prompts_func(examples):\n",
        "    texts  = examples[\"text\"]\n",
        "    outputs = []\n",
        "    for text in texts:\n",
        "         if text:\n",
        "            outputs.append(text + EOS_TOKEN)\n",
        "    return { \"text\" : outputs, }\n",
        "pass"
      ],
      "metadata": {
        "id": "8z6FSR7fgtPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "LtefWsNzh5B7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "full_dataset = load_dataset(\"uonlp/CulturaX\", \"ta\", split = \"train\")\n",
        "\n",
        "print(\"Step 1: Carving a small slice from the full dataset...\")\n",
        "small_slice = full_dataset.train_test_split(train_size=0.02, shuffle=True, seed=42)[\"train\"]\n",
        "\n",
        "\n",
        "print(\"Step 2: Splitting the small slice into train and eval sets...\")\n",
        "final_split = small_slice.train_test_split(test_size=0.5, shuffle=True, seed=42) # Split it 50/50\n",
        "\n",
        "train_dataset = final_split[\"train\"]\n",
        "eval_dataset  = final_split[\"test\"]\n",
        "\n",
        "\n",
        "print(\"Formatting datasets...\")\n",
        "train_dataset = train_dataset.map(formatting_prompts_func, batched = True)\n",
        "eval_dataset = eval_dataset.map(formatting_prompts_func, batched = True)\n",
        "\n",
        "print(f\"Final training set size: {len(train_dataset)}\")\n",
        "print(f\"Final evaluation set size: {len(eval_dataset)}\")"
      ],
      "metadata": {
        "id": "UaeGPd5xgw8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "from unsloth import UnslothTrainer, UnslothTrainingArguments\n",
        "\n",
        "trainer = UnslothTrainer(\n",
        "\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = train_dataset,\n",
        "    eval_dataset = eval_dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 4,\n",
        "\n",
        "    evaluation_strategy = \"steps\",\n",
        "    eval_steps = 50,\n",
        "    save_strategy = \"steps\",\n",
        "    save_steps = 50,\n",
        "    load_best_model_at_end = True,\n",
        "\n",
        "\n",
        "    args = UnslothTrainingArguments(\n",
        "         remove_unused_columns = False,\n",
        "        output_dir = \"/content/drive/MyDrive/My_CPT_Checkpoints\",\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 8,\n",
        "        max_steps = 120,\n",
        "        warmup_steps = 10,\n",
        "        learning_rate = 5e-5,\n",
        "        embedding_learning_rate = 1e-5,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        logging_steps = 10,\n",
        "        report_to = \"none\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(\"Trainer initialized successfully.\")"
      ],
      "metadata": {
        "id": "8If4cXdku61c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_training_iterations = 5\n",
        "\n",
        "for i in range(num_training_iterations):\n",
        "    print(f\"--- Starting Training Iteration {i+1}/{num_training_iterations} ---\")\n",
        "\n",
        "    print(f\"Creating a new random data subset with seed={i}...\")\n",
        "    dataset_subset = full_dataset.train_test_split(train_size=0.01, seed=i)[\"train\"]\n",
        "\n",
        "    dataset_subset = dataset_subset.remove_columns([\"url\", \"timestamp\", \"source\"])\n",
        "\n",
        "\n",
        "    trainer = UnslothTrainer(\n",
        "        model = model,\n",
        "        tokenizer = tokenizer,\n",
        "        train_dataset = dataset_subset,\n",
        "        dataset_text_field = \"text\",\n",
        "        max_seq_length = max_seq_length,\n",
        "        dataset_num_proc = 4,\n",
        "\n",
        "        args = UnslothTrainingArguments(\n",
        "            output_dir = \"/content/drive/MyDrive/My_CPT_Checkpoints\",\n",
        "            per_device_train_batch_size = 2,\n",
        "            gradient_accumulation_steps = 8,\n",
        "            max_steps = 120,\n",
        "            warmup_steps = 10,\n",
        "            learning_rate = 5e-5,\n",
        "            embedding_learning_rate = 1e-5,\n",
        "            optim = \"adamw_8bit\",\n",
        "            weight_decay = 0.01,\n",
        "            lr_scheduler_type = \"linear\",\n",
        "            seed = 3407,\n",
        "            logging_steps = 10,\n",
        "            remove_unused_columns = False,\n",
        "            report_to = \"none\",\n",
        "        ),\n",
        "    )\n",
        "\n",
        "\n",
        "    should_resume = True if i > 0 else False\n",
        "\n",
        "    print(f\"Starting trainer.train() with resume_from_checkpoint={should_resume}\")\n",
        "    trainer.train(resume_from_checkpoint = should_resume)\n",
        "\n",
        "print(\"--- Completed all training iterations! ---\")"
      ],
      "metadata": {
        "id": "QVa13WHn0MWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "cpt_model_path = \"/content/drive/MyDrive/My_Models/CPT_Model_Iter1\"\n",
        "\n",
        "\n",
        "model.save_pretrained(cpt_model_path)\n",
        "\n",
        "tokenizer.save_pretrained(cpt_model_path)\n",
        "\n",
        "print(\"Model saved successfully to Google Drive!\")"
      ],
      "metadata": {
        "id": "WP8MoD3TAEXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gc\n",
        "\n",
        "# 1. Delete the old trainer object that is holding onto the optimizer states\n",
        "del trainer\n",
        "print(\"Trainer object deleted.\")\n",
        "\n",
        "# 2. Run the Python garbage collector to free up CPU memory\n",
        "gc.collect()\n",
        "print(\"Garbage collection complete.\")\n",
        "\n",
        "# 3. Clear the GPU's memory cache\n",
        "torch.cuda.empty_cache()\n",
        "print(\"CUDA cache cleared. GPU memory should now be freed.\")"
      ],
      "metadata": {
        "id": "0H1ZevY4EmmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def tokenization_func(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True, max_length=max_seq_length, padding=\"max_length\")\n",
        "tokenized_eval_dataset = eval_dataset.map(tokenization_func, batched=True)\n",
        "\n",
        "\n",
        "print(\"Initializing a new trainer for evaluation...\")\n",
        "trainer_for_eval = UnslothTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = tokenized_eval_dataset,\n",
        "    args = UnslothTrainingArguments(\n",
        "        output_dir=\"temp_eval_dir\",\n",
        "        per_device_eval_batch_size = 1,\n",
        "        remove_unused_columns=False,\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(\"Evaluating...\")\n",
        "final_metrics = trainer_for_eval.evaluate(eval_dataset=tokenized_eval_dataset)\n",
        "final_perplexity = final_metrics['eval_perplexity']\n",
        "print(f\"\\nFinal Perplexity after CPT: {final_perplexity:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "7ekHMxmtEr6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"[Qualitative Analysis]\")\n",
        "prompts_to_test = [\n",
        "    \"திருக்குறளை எழுதியவர் யார்?\",\n",
        "    \"பொங்கல் பண்டிகை பற்றி ஒரு பந்தி எழுதுக.\",\n",
        "    \"கோவிட்-19 பற்றி 75-100 வார்த்தைகளுக்குள் ஒரு கட்டுரை தருக.\",\n",
        "    \"What is the capital of Japan?\",\n",
        "    \"Write a short sentence about a computer.\"\n",
        "]\n",
        "\n",
        "for prompt in prompts_to_test:\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    outputs = model.generate(**inputs, max_new_tokens=50, use_cache=True)\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    answer_only = response[len(prompt):].strip()\n",
        "\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "    print(f\"Answer: {answer_only}\")\n",
        "    print(\"-\" * 25)"
      ],
      "metadata": {
        "id": "4beAvCEqMYl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi\n",
        "\n",
        "api = HfApi(token=os.getenv(\"HF_TOKEN2\"))\n",
        "api.upload_folder(\n",
        "    folder_path=\"/content/drive/MyDrive/My_Models/CPT_Model_Iter1\",\n",
        "    repo_id=\"iCIIT/ThamizhiLLM\",\n",
        "    repo_type=\"model\",\n",
        ")\n"
      ],
      "metadata": {
        "id": "Jp-atRTV2MJL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
